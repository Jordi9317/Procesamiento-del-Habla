{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Clasificaci√≥n de Sentimientos con una Red Neuronal Multicapa (PyTorch)\n",
        "##üéØ Objetivo\n",
        "En esta actividad vas a construir una red neuronal feedforward multicapa (MLP) usando PyTorch. El objetivo es entrenarla para que pueda clasificar frases en espa√±ol como positivas o negativas.\n",
        "\n",
        "###Con esto vas a:\n",
        "\n",
        "* Comprender c√≥mo se arma una red con varias neuronas.\n",
        "\n",
        "* Usar funciones de activaci√≥n y entrenamiento autom√°tico.\n",
        "\n",
        "* Observar c√≥mo mejora respecto al perceptr√≥n simple de la Actividad 1."
      ],
      "metadata": {
        "id": "xPAukcLZZOqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üß∞ 1. Preparaci√≥n del entorno\n",
        "Importamos PyTorch y NumPy para comenzar."
      ],
      "metadata": {
        "id": "bp3C44_ubQAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XUvckjUsa0TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üóÇÔ∏è 2. Datos de entrenamiento\n",
        "Usamos un conjunto de frases t√≠picas de opiniones escritas en Argentina, etiquetadas como positivas (1) o negativas (0)."
      ],
      "metadata": {
        "id": "iuJrPnpWbUKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\n",
        "    \"La verdad, este lugar est√° b√°rbaro. Muy recomendable\",\n",
        "    \"Una porquer√≠a de servicio, nunca m√°s vuelvo\",\n",
        "    \"Me encant√≥ la comida, aunque la m√∫sica estaba muy fuerte\",\n",
        "    \"El env√≠o fue lento y el producto lleg√≥ da√±ado. Qu√© desastre\",\n",
        "    \"Todo excelente. Atenci√≥n de diez\",\n",
        "    \"Qu√© estafa, me arrepiento de haber comprado\",\n",
        "    \"Muy conforme con el resultado final\",\n",
        "    \"No me gust√≥ para nada la experiencia\",\n",
        "    \"Super√≥ mis expectativas, ¬°gracias!\",\n",
        "    \"No lo recomiendo, mala calidad\"\n",
        "]\n",
        "\n",
        "etiquetas = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])  # 1 = Positivo, 0 = Negativo\n"
      ],
      "metadata": {
        "id": "HFBeK5ZGbYF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üßæ 3. Construcci√≥n del vocabulario\n",
        "Definimos manualmente un vocabulario con palabras que suelen aparecer en frases de opini√≥n con carga positiva o negativa."
      ],
      "metadata": {
        "id": "fZAj_cxtba9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulario = [\n",
        "    \"b√°rbaro\", \"recomendable\", \"porquer√≠a\", \"nunca\", \"encant√≥\",\n",
        "    \"fuerte\", \"desastre\", \"excelente\", \"estafa\", \"arrepiento\",\n",
        "    \"conforme\", \"gust√≥\", \"super√≥\", \"gracias\", \"recomiendo\", \"mala\"\n",
        "]"
      ],
      "metadata": {
        "id": "1KEXTHNUbalR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##üß† 4. Preprocesamiento: vectorizaci√≥n de las frases\n",
        "Cada frase se convierte en un vector binario (bag-of-words) que indica si contiene alguna de las palabras del vocabulario."
      ],
      "metadata": {
        "id": "OP-pT2DTbk5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizar(frase, vocabulario):\n",
        "    tokens = frase.lower().split()\n",
        "    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario], dtype=np.float32)\n",
        "\n",
        "X_np = np.array([vectorizar(frase, vocabulario) for frase in frases], dtype=np.float32)\n",
        "y_np = etiquetas.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "# Convertimos a tensores de PyTorch\n",
        "X = torch.tensor(X_np)\n",
        "y = torch.tensor(y_np)"
      ],
      "metadata": {
        "id": "VaCF540vbnoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üß± 5. Definici√≥n del modelo (MLP)\n",
        "Vamos a crear un modelo simple con una capa oculta, activaci√≥n ReLU, y una salida sigmoide para predicci√≥n binaria."
      ],
      "metadata": {
        "id": "3rQdH7w3bsXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(vocabulario)\n",
        "hidden_size = 8\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "modelo = MLP()"
      ],
      "metadata": {
        "id": "sL_K2o15bvQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚öôÔ∏è 6. Entrenamiento del modelo\n",
        "Definimos la funci√≥n de p√©rdida y el optimizador. Entrenamos por varias √©pocas."
      ],
      "metadata": {
        "id": "h4-EQNcib3tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterio = nn.BCELoss()  # Binary Cross Entropy\n",
        "optimizador = optim.Adam(modelo.parameters(), lr=0.01)\n",
        "\n",
        "epocas = 200\n",
        "\n",
        "for epoca in range(epocas):\n",
        "    modelo.train()\n",
        "    salida = modelo(X)\n",
        "    loss = criterio(salida, y)\n",
        "\n",
        "    optimizador.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizador.step()\n",
        "\n",
        "    if (epoca + 1) % 10 == 0:\n",
        "        print(f\"√âpoca {epoca+1}, P√©rdida: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "2epH-XRHb58g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üß™ 7. Evaluaci√≥n con frases nuevas\n",
        "Probamos la red con frases que no estaban en el entrenamiento, para ver c√≥mo generaliza."
      ],
      "metadata": {
        "id": "YKZ5V6yhcDxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases_prueba = [\n",
        "    \"No me gust√≥ la atenci√≥n, bastante mala\",\n",
        "    \"Muy buena experiencia, todo excelente\",\n",
        "    \"Una estafa total, no lo recomiendo\",\n",
        "    \"S√∫per conforme con el servicio\",\n",
        "    \"Nada que ver con lo prometido, una decepci√≥n\"\n",
        "]\n",
        "\n",
        "# Vectorizamos las frases de prueba\n",
        "X_prueba_np = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba], dtype=np.float32)\n",
        "X_prueba = torch.tensor(X_prueba_np)\n",
        "\n",
        "# Predicci√≥n\n",
        "modelo.eval()\n",
        "with torch.no_grad():\n",
        "    predicciones = modelo(X_prueba)\n",
        "\n",
        "# Mostrar resultados\n",
        "for frase, pred in zip(frases_prueba, predicciones):\n",
        "    clase = \"Positivo\" if pred.item() >= 0.5 else \"Negativo\"\n",
        "    print(f\"Frase: '{frase}' => Sentimiento predicho: {clase} ({pred.item():.2f})\")"
      ],
      "metadata": {
        "id": "gpIGfi-tcFPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üí¨ Reflexi√≥n final\n",
        "###üëâ ¬øQu√© aprendimos?\n",
        "\n",
        "* C√≥mo implementar y entrenar una red neuronal multicapa para an√°lisis de sentimiento.\n",
        "\n",
        "* C√≥mo preprocesar texto en espa√±ol usando bag-of-words.\n",
        "\n",
        "* Las ventajas del MLP frente al perceptr√≥n simple.\n",
        "\n",
        "* Limitaciones: a√∫n no capta el orden de las palabras ni el contexto secuencial.\n",
        "\n",
        "‚û°Ô∏è En la pr√≥xima actividad aprenderemos a usar redes recurrentes (LSTM) para incorporar secuencia y memoria en el procesamiento de texto. ¬°Nos acercamos a modelos m√°s cercanos al lenguaje humano!"
      ],
      "metadata": {
        "id": "138SwAUvcRnQ"
      }
    }
  ]
}