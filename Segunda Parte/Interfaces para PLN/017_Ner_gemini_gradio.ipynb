{
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Reconocimiento de Entidades Nombradas\n",
        "## Ejercicio Pr√°ctico - Procesamiento de Lenguaje Natural\n",
        "\n",
        "**Objetivos de Aprendizaje:**\n",
        "1. Implementar NER usando modelos pre-entrenados en espa√±ol\n",
        "2. Crear interfaces interactivas con Gradio\n",
        "3. Comparar enfoques: Transformers vs API Gemini\n",
        "4. Desarrollar prototipos r√°pidos para aplicaciones de PLN\n",
        "\n",
        "---\n",
        "**Entorno recomendado:** Google Colab o Amazon SageMaker Studio\n",
        "\n",
        "**Tiempo estimado:** 60-90 minutos"
      ],
      "id": "title_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## Instalaci√≥n de Dependencias"
      ],
      "id": "setup_section"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Instalaci√≥n de librer√≠as necesarias\n",
        "%%capture\n",
        "!pip install -q transformers torch gradio google-genai"
      ],
      "id": "install_deps"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar instalaci√≥n\n",
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(\"Todas las dependencias instaladas correctamente\")"
      ],
      "metadata": {
        "id": "cJksfScFfyjN",
        "outputId": "b7dc139a-df9a-40fb-a44f-f4b24b762458",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cJksfScFfyjN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Todas las dependencias instaladas correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_section"
      },
      "source": [
        "## Configuraci√≥n de APIs\n",
        "\n",
        "### Para Google Colab:\n",
        "1. And√° a la barra lateral izquierda y hac√© clic en üîë (Secrets)\n",
        "2. Agrega una nueva clave: `GOOGLE_API_KEY`\n",
        "3. Pega tu API key de Google AI Studio\n",
        "\n",
        "### Para SageMaker Studio:\n",
        "1. Configura las variables de entorno en tu instancia\n",
        "2. O usa el m√©todo de input manual m√°s abajo"
      ],
      "id": "config_section"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "config_api",
        "outputId": "e116f94c-bde8-4a6b-8dbc-d74338822c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key cargada desde Google Colab Secrets\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de API Key para Gemini\n",
        "try:\n",
        "    # M√©todo 1: Google Colab Secrets\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"API Key cargada desde Google Colab Secrets\")\n",
        "except:\n",
        "    # M√©todo 2: Variables de entorno (SageMaker)\n",
        "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
        "    if GOOGLE_API_KEY:\n",
        "        print(\"API Key cargada desde variables de entorno\")\n",
        "    else:\n",
        "        print(\"No se encontr√≥ GOOGLE_API_KEY\")\n",
        "        print(\"Podes continuar solo con la parte de Transformers\")"
      ],
      "id": "config_api"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "---\n",
        "# PARTE 1: NER con Transformers de Hugging Face\n",
        "\n",
        "Utilizaremos un modelo especializado en espa√±ol que puede identificar:\n",
        "- **PER**: Personas\n",
        "- **LOC**: Lugares\n",
        "- **ORG**: Organizaciones  \n",
        "- **MISC**: Miscel√°neo"
      ],
      "id": "part1_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_transformer_model"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"üñ•Ô∏è  Dispositivo: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# Cargar modelo de NER en espa√±ol\n",
        "print(\"üì• Cargando modelo de NER para espa√±ol...\")\n",
        "MODEL_NAME = \"mrm8488/bert-spanish-cased-finetuned-ner\"\n",
        "\n",
        "try:\n",
        "    ner_pipeline = pipeline(\n",
        "        \"ner\",\n",
        "        model=MODEL_NAME,\n",
        "        aggregation_strategy=\"simple\",\n",
        "        device=device\n",
        "    )\n",
        "    print(f\"Modelo {MODEL_NAME} cargado exitosamente\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al cargar modelo: {e}\")\n",
        "    ner_pipeline = None"
      ],
      "id": "load_transformer_model"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "test_transformer_ner"
      },
      "outputs": [],
      "source": [
        "# Texto de ejemplo con contexto argentino\n",
        "texto_ejemplo = \"\"\"\n",
        "Hola, soy Mar√≠a Gonz√°lez y trabajo en la Universidad de Buenos Aires.\n",
        "Vivo en el barrio de San Telmo y mi empresa favorita es MercadoLibre.\n",
        "La semana pasada visit√© el Obelisco con mi amigo Carlos P√©rez,\n",
        "quien trabaja en Google Argentina. Nos encontramos en la estaci√≥n\n",
        "Constituci√≥n del subte y fuimos a comer un asado en La Boca.\n",
        "\"\"\""
      ],
      "id": "test_transformer_ner"
    },
    {
      "cell_type": "code",
      "source": [
        "def analizar_entidades_transformers(texto):\n",
        "    \"\"\"Procesa texto y extrae entidades usando Transformers\"\"\"\n",
        "    if not ner_pipeline:\n",
        "        return []\n",
        "\n",
        "    entidades = ner_pipeline(texto)\n",
        "\n",
        "    # Formatear resultados\n",
        "    resultados = []\n",
        "    for ent in entidades:\n",
        "        resultados.append({\n",
        "            'texto': ent['word'],\n",
        "            'etiqueta': ent['entity_group'],\n",
        "            'confianza': round(ent['score'], 3),\n",
        "            'posicion': (ent['start'], ent['end'])\n",
        "        })\n",
        "\n",
        "    return resultados"
      ],
      "metadata": {
        "id": "v40sR-IKYXyA"
      },
      "id": "v40sR-IKYXyA",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar el modelo\n",
        "print(\"üîç Analizando texto de ejemplo...\")\n",
        "print(f\"üìù Texto: {texto_ejemplo.strip()}\")\n",
        "print(\"\\nüìä Entidades encontradas:\")\n",
        "\n",
        "entidades_encontradas = analizar_entidades_transformers(texto_ejemplo)\n",
        "for ent in entidades_encontradas:\n",
        "    print(f\"  ‚Ä¢ {ent['texto']} ‚Üí {ent['etiqueta']} (confianza: {ent['confianza']})\")"
      ],
      "metadata": {
        "id": "lh7rDIQjYXtj",
        "outputId": "d13f28ad-dadd-4e73-a517-ac171df7752f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "id": "lh7rDIQjYXtj",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Analizando texto de ejemplo...\n",
            "üìù Texto: Hola, soy Mar√≠a Gonz√°lez y trabajo en la Universidad de Buenos Aires.\n",
            "Vivo en el barrio de San Telmo y mi empresa favorita es MercadoLibre.\n",
            "La semana pasada visit√© el Obelisco con mi amigo Carlos P√©rez,\n",
            "quien trabaja en Google Argentina. Nos encontramos en la estaci√≥n\n",
            "Constituci√≥n del subte y fuimos a comer un asado en La Boca.\n",
            "\n",
            "üìä Entidades encontradas:\n",
            "  ‚Ä¢ Mar√≠a Gonz√°lez ‚Üí PER (confianza: 0.9990000128746033)\n",
            "  ‚Ä¢ Universidad de Buenos Aires ‚Üí ORG (confianza: 0.9990000128746033)\n",
            "  ‚Ä¢ San Telmo ‚Üí LOC (confianza: 0.9980000257492065)\n",
            "  ‚Ä¢ MercadoLibre ‚Üí ORG (confianza: 0.996999979019165)\n",
            "  ‚Ä¢ Obelisco ‚Üí LOC (confianza: 0.9959999918937683)\n",
            "  ‚Ä¢ Carlos P√©rez ‚Üí PER (confianza: 1.0)\n",
            "  ‚Ä¢ Google Argentina ‚Üí ORG (confianza: 0.9860000014305115)\n",
            "  ‚Ä¢ Constituci√≥n ‚Üí LOC (confianza: 0.9950000047683716)\n",
            "  ‚Ä¢ La Boca ‚Üí LOC (confianza: 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "---\n",
        "# PARTE 2: NER con API de Gemini\n",
        "\n",
        "Utilizaremos la API de Gemini para un an√°lisis m√°s detallado y contextual."
      ],
      "id": "part2_header"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "setup_gemini",
        "outputId": "eeb6df79-ad76-4661-a2ce-686e08a8060f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cliente Gemini configurado correctamente\n"
          ]
        }
      ],
      "source": [
        "# Configurar cliente Gemini\n",
        "cliente_gemini = None\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    try:\n",
        "        from google import genai\n",
        "        cliente_gemini = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Cliente Gemini configurado correctamente\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al configurar Gemini: {e}\")\n",
        "else:\n",
        "    print(\"API Key de Gemini no disponible\")\n",
        "    print(\"Podes obtener una gratis en: https://ai.google.dev/\")"
      ],
      "id": "setup_gemini"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "test_gemini_ner",
        "outputId": "ade7321c-995f-45d4-92cb-343277b93c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Analizando con Gemini...\n",
            "\n",
            "An√°lisis de Gemini:\n",
            "*   Mar√≠a Gonz√°lez ‚Üí PERSONA ‚Üí Nombre de la persona que habla.\n",
            "*   Universidad de Buenos Aires ‚Üí ORGANIZACI√ìN ‚Üí Nombre de la universidad donde trabaja.\n",
            "*   San Telmo ‚Üí LUGAR ‚Üí Nombre del barrio donde vive.\n",
            "*   MercadoLibre ‚Üí ORGANIZACI√ìN ‚Üí Empresa favorita de la persona.\n",
            "*   Obelisco ‚Üí LUGAR ‚Üí Monumento de la Ciudad de Buenos Aires.\n",
            "*   Carlos P√©rez ‚Üí PERSONA ‚Üí Nombre del amigo de la persona.\n",
            "*   Google Argentina ‚Üí ORGANIZACI√ìN ‚Üí Filial argentina de la empresa Google.\n",
            "*   Constituci√≥n ‚Üí LUGAR ‚Üí Nombre de la estaci√≥n de subte.\n",
            "*   La Boca ‚Üí LUGAR ‚Üí Barrio de la Ciudad de Buenos Aires.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def analizar_entidades_gemini(texto):\n",
        "    \"\"\"Analiza entidades usando Gemini API\"\"\"\n",
        "    if not cliente_gemini:\n",
        "        return \"‚ùå Cliente Gemini no disponible\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Extra√© todas las entidades nombradas del siguiente texto en espa√±ol argentino y clasific√°las:\n",
        "\n",
        "    CATEGOR√çAS:\n",
        "    - PERSONA: Nombres de personas\n",
        "    - LUGAR: Ciudades, pa√≠ses, barrios, direcciones, lugares espec√≠ficos\n",
        "    - ORGANIZACI√ìN: Empresas, universidades, instituciones\n",
        "    - MISCEL√ÅNEO: Otros nombres propios (productos, eventos, marcas)\n",
        "\n",
        "    FORMATO DE RESPUESTA:\n",
        "    [ENTIDAD] ‚Üí [CATEGOR√çA] ‚Üí [BREVE EXPLICACI√ìN]\n",
        "\n",
        "    TEXTO A ANALIZAR:\n",
        "    {texto}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        respuesta = cliente_gemini.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=[prompt]\n",
        "        )\n",
        "        return respuesta.text\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {e}\"\n",
        "\n",
        "# Probar Gemini si est√° disponible\n",
        "if cliente_gemini:\n",
        "    print(\"üîç Analizando con Gemini...\")\n",
        "    resultado_gemini = analizar_entidades_gemini(texto_ejemplo)\n",
        "    print(\"\\nAn√°lisis de Gemini:\")\n",
        "    print(resultado_gemini)\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Saltando an√°lisis con Gemini (API Key no disponible)\")"
      ],
      "id": "test_gemini_ner"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_header"
      },
      "source": [
        "---\n",
        "# PARTE 3: Interfaces Interactivas con Gradio\n",
        "\n",
        "Crearemos interfaces web interactivas para probar nuestros modelos."
      ],
      "id": "part3_header"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gradio_transformers",
        "outputId": "afd982ea-7d25-41ba-d6f4-bfdc106c91a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interfaz de Transformers creada\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def interfaz_ner_transformers(texto):\n",
        "    \"\"\"Interfaz para el modelo de Transformers\"\"\"\n",
        "    if not texto.strip():\n",
        "        return {\"text\": \"Ingresa un texto para analizar\", \"entities\": []}\n",
        "\n",
        "    if not ner_pipeline:\n",
        "        return {\"text\": \"Modelo no disponible\", \"entities\": []}\n",
        "\n",
        "    # Procesar con Transformers\n",
        "    entidades = ner_pipeline(texto)\n",
        "\n",
        "    # Formatear para Gradio HighlightedText\n",
        "    entidades_gradio = []\n",
        "    for ent in entidades:\n",
        "        entidades_gradio.append({\n",
        "            \"entity\": ent[\"entity_group\"],\n",
        "            \"word\": ent[\"word\"],\n",
        "            \"start\": ent[\"start\"],\n",
        "            \"end\": ent[\"end\"],\n",
        "            \"score\": ent[\"score\"]\n",
        "        })\n",
        "\n",
        "    return {\"text\": texto, \"entities\": entidades_gradio}\n",
        "\n",
        "# Ejemplos para la interfaz\n",
        "ejemplos_arg = [\n",
        "    \"Me llamo Juan P√©rez y trabajo en el Banco Naci√≥n en Buenos Aires.\",\n",
        "    \"Cristina Kirchner fue presidenta de Argentina y vive en Santa Cruz.\",\n",
        "    \"River Plate jugar√° contra Boca Juniors en el estadio Monumental.\",\n",
        "    \"Lionel Messi naci√≥ en Rosario y jug√≥ en el Barcelona.\",\n",
        "    \"La Universidad de La Plata es muy prestigiosa en Argentina.\"\n",
        "]\n",
        "\n",
        "# Crear interfaz\n",
        "demo_transformers = gr.Interface(\n",
        "    fn=interfaz_ner_transformers,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"üìù Texto a analizar\",\n",
        "            placeholder=\"Escribe aqu√≠ tu texto en espa√±ol...\",\n",
        "            lines=4\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.HighlightedText(\n",
        "            label=\"üéØ Entidades Identificadas\",\n",
        "            show_legend=True\n",
        "        )\n",
        "    ],\n",
        "    title=\"NER con Transformers - Espa√±ol (de argentina)\",\n",
        "    description=\"\"\"\n",
        "    **Modelo:** `mrm8488/bert-spanish-cased-finetuned-ner`\n",
        "\n",
        "    Identifica entidades nombradas en textos en espa√±ol:\n",
        "    - üßë **PER**: Personas\n",
        "    - üåç **LOC**: Lugares\n",
        "    - üè¢ **ORG**: Organizaciones\n",
        "    - üì¶ **MISC**: Miscel√°neo\n",
        "    \"\"\",\n",
        "    examples=ejemplos_arg,\n",
        "    allow_flagging=\"never\",\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Interfaz de Transformers creada\")"
      ],
      "id": "gradio_transformers"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gradio_gemini",
        "outputId": "ef162842-e4b1-4bfe-9f31-bcf91209ecc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interfaz de Gemini creada\n"
          ]
        }
      ],
      "source": [
        "# Interfaz para Gemini (solo si est√° disponible)\n",
        "if cliente_gemini:\n",
        "    def interfaz_ner_gemini(texto):\n",
        "        \"\"\"Interfaz para Gemini API\"\"\"\n",
        "        if not texto.strip():\n",
        "            return \"Ingresa un texto para analizar\"\n",
        "        return analizar_entidades_gemini(texto)\n",
        "\n",
        "    demo_gemini = gr.Interface(\n",
        "        fn=interfaz_ner_gemini,\n",
        "        inputs=[\n",
        "            gr.Textbox(\n",
        "                label=\"üìù Texto a analizar\",\n",
        "                placeholder=\"Escribe aqu√≠ tu texto en espa√±ol...\",\n",
        "                lines=4\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Textbox(\n",
        "                label=\"üß† An√°lisis de Gemini\",\n",
        "                lines=10\n",
        "            )\n",
        "        ],\n",
        "        title=\"NER con Gemini - An√°lisis Detallado\",\n",
        "        description=\"\"\"\n",
        "        **Modelo:** Google Gemini 2.0 Flash\n",
        "\n",
        "        An√°lisis avanzado de entidades nombradas con explicaciones contextuales\n",
        "        optimizado para espa√±ol argentino.\n",
        "        \"\"\",\n",
        "        examples=ejemplos_arg,\n",
        "        allow_flagging=\"never\",\n",
        "        theme=gr.themes.Soft()\n",
        "    )\n",
        "    print(\"‚úÖ Interfaz de Gemini creada\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Interfaz de Gemini no creada (API Key no disponible)\")"
      ],
      "id": "gradio_gemini"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gradio_comparison",
        "outputId": "264cb2ca-573b-48a7-a512-3e1f5d7f6ed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interfaz comparativa creada\n"
          ]
        }
      ],
      "source": [
        "# Interfaz comparativa (solo si ambos est√°n disponibles)\n",
        "if ner_pipeline and cliente_gemini:\n",
        "    def comparar_modelos(texto):\n",
        "        \"\"\"Compara resultados de ambos modelos\"\"\"\n",
        "        if not texto.strip():\n",
        "            return \"Ingresa texto para comparar\", \"Ingresa texto para comparar\"\n",
        "\n",
        "        # Resultado Transformers\n",
        "        entidades_tf = analizar_entidades_transformers(texto)\n",
        "        resultado_tf = \"TRANSFORMERS:\\n\\n\"\n",
        "        for ent in entidades_tf:\n",
        "            resultado_tf += f\"‚Ä¢ {ent['texto']} ‚Üí {ent['etiqueta']} (confianza: {ent['confianza']})\\n\"\n",
        "\n",
        "        # Resultado Gemini\n",
        "        resultado_gemini = \"GEMINI:\\n\\n\" + analizar_entidades_gemini(texto)\n",
        "\n",
        "        return resultado_tf, resultado_gemini\n",
        "\n",
        "    demo_comparativo = gr.Interface(\n",
        "        fn=comparar_modelos,\n",
        "        inputs=[\n",
        "            gr.Textbox(\n",
        "                label=\"üìù Texto a comparar\",\n",
        "                placeholder=\"Ingresa texto para ver la comparaci√≥n...\",\n",
        "                lines=3\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Transformers\", lines=8),\n",
        "            gr.Textbox(label=\"Gemini\", lines=8)\n",
        "        ],\n",
        "        title=\"‚öîÔ∏è Comparaci√≥n: Transformers vs Gemini\",\n",
        "        description=\"Compara los resultados de ambos enfoques lado a lado.\",\n",
        "        examples=[\n",
        "            \"Diego Maradona jug√≥ en Boca Juniors y en el Napoli de Italia.\",\n",
        "            \"El gobierno argentino anunci√≥ medidas desde Casa Rosada.\"\n",
        "        ],\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "    print(\"‚úÖ Interfaz comparativa creada\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Interfaz comparativa no creada (requiere ambos modelos)\")"
      ],
      "id": "gradio_comparison"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "launch_section"
      },
      "source": [
        "## üöÄ Lanzar Interfaces\n",
        "\n",
        "Ejecuta las celdas siguientes para lanzar las interfaces interactivas:"
      ],
      "id": "launch_section"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "launch_transformers",
        "outputId": "a70aa196-8ada-4e9b-9d1b-798e7a0e4529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Lanzando interfaz de Transformers...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8ff6be46dc625d3d35.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8ff6be46dc625d3d35.gradio.live\" width=\"100%\" height=\"600\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Lanzar interfaz de Transformers\n",
        "if ner_pipeline:\n",
        "    print(\"üöÄ Lanzando interfaz de Transformers...\")\n",
        "    demo_transformers.launch(share=True, height=600)\n",
        "else:\n",
        "    print(\"‚ùå No se puede lanzar: modelo de Transformers no disponible\")"
      ],
      "id": "launch_transformers"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "launch_gemini",
        "outputId": "4501e9cb-2a26-4731-a46f-04dafdfc0c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Lanzando interfaz de Gemini...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9b8c08c2a919dfdafc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9b8c08c2a919dfdafc.gradio.live\" width=\"100%\" height=\"600\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Lanzar interfaz de Gemini\n",
        "if cliente_gemini:\n",
        "    print(\"üöÄ Lanzando interfaz de Gemini...\")\n",
        "    demo_gemini.launch(share=True, height=600)\n",
        "else:\n",
        "    print(\"‚ùå No se puede lanzar: API de Gemini no disponible\")"
      ],
      "id": "launch_gemini"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "launch_comparison",
        "outputId": "c6c56a43-3074-4c4a-a4e6-e23ff0a09de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Lanzando interfaz comparativa...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://98f71e08552ea80962.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://98f71e08552ea80962.gradio.live\" width=\"100%\" height=\"600\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Lanzar interfaz comparativa\n",
        "if ner_pipeline and cliente_gemini:\n",
        "    print(\"üöÄ Lanzando interfaz comparativa...\")\n",
        "    demo_comparativo.launch(share=True, height=600)\n",
        "else:\n",
        "    print(\"‚ùå No se puede lanzar: requiere ambos modelos disponibles\")"
      ],
      "id": "launch_comparison"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# üéì EJERCICIOS\n",
        "\n",
        "## üìù Ejercicio 1: Personalizaci√≥n (B√ÅSICO)\n",
        "1. Modifica los ejemplos para incluir m√°s contexto argentino espec√≠fico\n",
        "2. Agrega 3 ejemplos nuevos con nombres de barrios porte√±os\n",
        "3. Prob√° con texto de diferentes regiones de Argentina\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2K-Bt9ZtVN2"
      },
      "id": "Y2K-Bt9ZtVN2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_section"
      },
      "source": [
        "\n",
        "\n",
        "## ü§î Preguntas de Reflexi√≥n\n",
        "1. ¬øCu√°les son las ventajas y desventajas de cada enfoque?\n",
        "# **Transformers**\n",
        "\n",
        "**Ventajas:**\n",
        "\n",
        "*1)Privacidad y Control*: Los datos se procesan localmente (en tu entorno de Colab, aunque el modelo se descarga), lo que es importante si trabajas con informaci√≥n sensible que no puede salir de tu control.\n",
        "\n",
        "*2)Costo*: Una vez que el modelo est√° descargado, el costo de inferencia es nulo o muy bajo (solo el costo computacional de tu entorno). No pagas por cada llamada a una API.\n",
        "\n",
        "*3)Personalizaci√≥n*: Aunque usamos un modelo pre-entrenado, podr√≠as afinarlo con tus propios datos si necesitaras identificar entidades espec√≠ficas para tu dominio.\n",
        "\n",
        "*4)Velocidad*: Para procesamiento por lotes grande, puede ser m√°s r√°pido que enviar y recibir datos de una API remota, especialmente si tienes acceso a GPU.\n",
        "\n",
        "**Desventajas:**\n",
        "\n",
        "*1)Configuraci√≥n*: Requiere la instalaci√≥n de librer√≠as (Transformers, PyTorch/TensorFlow) y la descarga del modelo, lo que a√±ade complejidad inicial.\n",
        "\n",
        "*2)Recursos Computacionales*: Necesitas hardware suficiente (CPU o GPU) para cargar y ejecutar el modelo, especialmente modelos grandes.\n",
        "\n",
        "*3)Mantenimiento*: Eres responsable de mantener las librer√≠as y el modelo actualizados.\n",
        "\n",
        "*4)Flexibilidad/Generalidad*: Los modelos pre-entrenados tienen etiquetas fijas (PER, LOC, ORG, MISC en este caso). Si necesitas identificar otros tipos de entidades, tendr√≠as que re-entrenar o usar un modelo diferente.\n",
        "\n",
        "# **API de Gemini**\n",
        "\n",
        "**Ventajas:**\n",
        "\n",
        "*1)Facilidad de Uso*: La integraci√≥n es muy simple (vimos que solo necesitas una API Key y un par de l√≠neas de c√≥digo para hacer la llamada). No necesitas preocuparte por la infraestructura o la descarga de modelos grandes.\n",
        "\n",
        "*2)Modelos Potentes y Actualizados*: Tienes acceso a modelos de lenguaje de vanguardia que Google mantiene y mejora continuamente.\n",
        "\n",
        "*3)Flexibilidad (Few-shot/Zero-shot)*: Como viste en el prompt que usamos, puedes instruir a Gemini con lenguaje natural para identificar entidades y dar explicaciones. Esto permite adaptarte a nuevos tipos de entidades o formatos de salida sin necesidad de re-entrenar.\n",
        "\n",
        "*4)Contexto y Explicaci√≥n*: Gemini puede proporcionar un an√°lisis m√°s contextual y explicaciones sobre la entidad encontrada, algo que un modelo NER cl√°sico no suele hacer por defecto (vimos c√≥mo nos devolvi√≥ \"[ENTIDAD] ‚Üí [CATEGOR√çA] ‚Üí [BREVE EXPLICACI√ìN]\").\n",
        "\n",
        "**Desventajas:**\n",
        "\n",
        "*1)Costo*: Generalmente pagas por uso (por cantidad de tokens procesados o llamadas a la API). Esto puede ser costoso para procesar grandes vol√∫menes de texto.\n",
        "\n",
        "*2)Latencia*: Hay una latencia asociada a enviar la solicitud a trav√©s de la red y esperar la respuesta.\n",
        "\n",
        "*3)Privacidad*: La informaci√≥n se env√≠a a trav√©s de internet a un servicio de terceros. Debes asegurarte de cumplir con las pol√≠ticas de uso y privacidad, especialmente si trabajas con datos confidenciales.\n",
        "\n",
        "*4)Dependencia del Proveedor*: Dependes de la disponibilidad y las condiciones del servicio de la API.\n"
      ],
      "id": "exercises_section"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. ¬øEn qu√© casos usar√≠as un modelo local vs una API?**\n",
        "\n",
        "**Usar Modelo Local (Transformers):**\n",
        "\n",
        "Cuando la privacidad de los datos es cr√≠tica.\n",
        "Cuando necesitas procesar grandes vol√∫menes de texto de forma continua y el costo por llamada a la API es prohibitivo.\n",
        "Cuando requieres baja latencia y el procesamiento debe ser muy r√°pido.\n",
        "Cuando necesitas control total sobre el modelo y la infraestructura.\n",
        "Si tienes recursos computacionales (especialmente GPU) disponibles.\n",
        "Si necesitas afinar el modelo con datos espec√≠ficos de tu dominio.\n",
        "\n",
        "**Usar API (Gemini):**\n",
        "\n",
        "Cuando la facilidad de implementaci√≥n y la rapidez para obtener resultados son prioritarias.\n",
        "Para prototipado r√°pido y experimentaci√≥n con diferentes tareas de PLN.\n",
        "Cuando necesitas flexibilidad para identificar diferentes tipos de entidades o adaptar el formato de salida f√°cilmente (zero-shot/few-shot learning).\n",
        "Para an√°lisis que requieren comprensi√≥n contextual o explicaciones detalladas.\n",
        "Si no tienes acceso a hardware potente para ejecutar modelos grandes localmente.\n",
        "Para vol√∫menes de procesamiento moderados donde el costo por llamada es aceptable.\n",
        "\n"
      ],
      "metadata": {
        "id": "ewBjB1cttcVm"
      },
      "id": "ewBjB1cttcVm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. ¬øC√≥mo evaluar√≠as la precisi√≥n de los resultados?**\n",
        "\n",
        "Evaluar la precisi√≥n de un modelo NER requiere un conjunto de datos de \"verdad fundamental\" (ground truth), es decir, texto donde las entidades nombradas ya han sido anotadas manualmente por humanos.\n",
        "\n",
        "**Los pasos t√≠picos ser√≠an:**\n",
        "\n",
        "1)Crear un conjunto de prueba: Toma un corpus de texto representativo de tu dominio y haz que anotadores humanos identifiquen y etiqueten todas las entidades de inter√©s (PER, LOC, ORG, MISC, etc.).\n",
        "\n",
        "2)Procesar el conjunto de prueba: Ejecuta el modelo NER (Transformers o Gemini) sobre este mismo texto sin las anotaciones.\n",
        "\n",
        "3)Comparar resultados: Compara las entidades identificadas por el modelo con las anotaciones humanas. Puedes usar m√©tricas como:\n",
        "\n",
        "  Precision: De las entidades que el modelo identific√≥, ¬øcu√°ntas son correctas?\n",
        "\n",
        "  Recall: De todas las entidades reales en el texto, ¬øcu√°ntas identific√≥ el modelo?\n",
        "\n",
        "  F1-score: Es la media arm√≥nica de Precision y Recall, dando un equilibrio entre ambas. Es la m√©trica m√°s com√∫n para NER.\n",
        "\n",
        "Tambi√©n podr√≠as evaluar si la categor√≠a asignada es correcta y si los l√≠mites (start/end) de la entidad son precisos.\n",
        "\n",
        "4)An√°lisis de Errores: Revisa los errores (falsos positivos, falsos negativos, errores de categor√≠a) para entender por qu√© el modelo falla y d√≥nde se puede mejorar.\n",
        "\n",
        "En el caso de Gemini, como no te da una \"confianza\" num√©rica por entidad como Transformers, la evaluaci√≥n se basar√≠a m√°s en la comparaci√≥n directa de las entidades y categor√≠as extra√≠das con el ground truth.\n",
        "\n"
      ],
      "metadata": {
        "id": "PhJZwDYVv7pG"
      },
      "id": "PhJZwDYVv7pG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ¬øQu√© consideraciones √©ticas debemos tener en cuenta?**\n",
        "\n",
        "El uso de NER, especialmente con datos sensibles, conlleva importantes consideraciones √©ticas:\n",
        "\n",
        "**1)Privacidad**: Procesar nombres de personas (PER) o ubicaciones espec√≠ficas (LOC) puede implicar manejar informaci√≥n personal identificable (PII). Debes asegurarte de cumplir con las regulaciones de protecci√≥n de datos (como GDPR o leyes locales) y anonimizar o pseudonimizar los datos si es necesario.\n",
        "\n",
        "**2)Sesgos**: Los modelos de lenguaje se entrenan con grandes cantidades de texto y pueden heredar y perpetuar sesgos presentes en esos datos (por ejemplo, asociar ciertos nombres con ciertas ocupaciones o g√©neros de forma estereotipada). Esto puede afectar la precisi√≥n o ser injusto.\n",
        "\n",
        "**3)Uso Malintencionado**: La capacidad de extraer PII de forma automatizada podr√≠a usarse para vigilancia, creaci√≥n de perfiles no deseados o desinformaci√≥n.\n",
        "\n",
        "**4)Transparencia y Explicabilidad**: Con modelos m√°s complejos como Gemini, puede ser dif√≠cil entender por qu√© identific√≥ una entidad de cierta manera. La falta de transparencia puede ser un problema en aplicaciones cr√≠ticas.\n",
        "\n",
        "**5)Responsabilidad**: ¬øQui√©n es responsable si el sistema NER comete un error con consecuencias negativas (por ejemplo, omitir una entidad cr√≠tica en un informe m√©dico)?\n",
        "\n"
      ],
      "metadata": {
        "id": "eZzmMEmRwhf-"
      },
      "id": "eZzmMEmRwhf-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. ¬øC√≥mo escalar√≠as esta soluci√≥n para procesar miles de documentos?**\n",
        "\n",
        "Escalar la soluci√≥n depender√≠a del enfoque elegido:\n",
        "\n",
        "**Para el enfoque de Transformers (modelo local):**\n",
        "\n",
        "Procesamiento Distribuido:\n",
        "\n",
        "Paralelismo: Si tienes muchos documentos, puedes dividirlos y procesarlos en paralelo en m√∫ltiples m√°quinas o n√∫cleos de CPU/GPU.\n",
        "\n",
        "Frameworks: Usar frameworks de procesamiento distribuido como Apache Spark o Dask para manejar grandes vol√∫menes de datos y coordinar el procesamiento en un cluster.\n",
        "\n",
        "Hardware: Utilizar m√°quinas con m√°s CPU, m√°s RAM o m√∫ltiples GPU de alto rendimiento.\n",
        "\n",
        "Optimizaci√≥n del C√≥digo: Asegurarse de que el c√≥digo de procesamiento sea eficiente y aproveche al m√°ximo el hardware disponible (por ejemplo, procesar en lotes (batches) en la GPU).\n",
        "\n",
        "Despliegue: Desplegar el modelo en un entorno escalable, como un cluster de Kubernetes, un servicio de serverless computing (AWS Lambda, Google Cloud Functions) con provisionamiento adecuado, o una plataforma de MLOps que maneje el escalado autom√°tico basado en la carga.\n",
        "\n",
        "**Para el enfoque de API (Gemini):**\n",
        "\n",
        "Aumentar Cuota/L√≠mites: Contactar al proveedor de la API (Google) para aumentar los l√≠mites de llamadas por minuto/hora si tu volumen de procesamiento excede los l√≠mites est√°ndar.\n",
        "\n",
        "Procesamiento As√≠ncrono: Enviar solicitudes a la API de forma as√≠ncrona (en lugar de esperar cada respuesta antes de enviar la siguiente) para maximizar el rendimiento y reducir el tiempo total.\n",
        "\n",
        "Procesamiento por Lotes (si la API lo soporta): Algunas APIs permiten enviar m√∫ltiples textos en una sola solicitud, lo que reduce la latencia y el n√∫mero total de llamadas.\n",
        "\n",
        "Manejo de Errores y Reintentos: Implementar una l√≥gica robusta para manejar errores de red o de la API y reintentar las llamadas fallidas.\n",
        "\n",
        "Monitoreo de Costos: Implementar monitoreo continuo de los costos de la API para asegurarte de que se mantengan dentro del presupuesto.\n",
        "\n",
        "En resumen, escalar Transformers implica escalar tu propia infraestructura computacional y optimizar el c√≥digo para el procesamiento local, mientras que escalar con una API implica gestionar el uso de la API, los costos y la comunicaci√≥n eficiente con el servicio remoto."
      ],
      "metadata": {
        "id": "P963-2ebxaeu"
      },
      "id": "P963-2ebxaeu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Ejercicio 2: An√°lisis Comparativo (INTERMEDIO)\n",
        "1. Crea una funci√≥n que cuente cu√°ntas entidades encuentra cada modelo\n",
        "2. Implementa un sistema de m√©tricas de tiempo de procesamiento\n",
        "3. Analiza en qu√© casos cada modelo funciona mejor"
      ],
      "metadata": {
        "id": "7ciRnGtLx8Ql"
      },
      "id": "7ciRnGtLx8Ql"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "exercise_template",
        "outputId": "e1dc0f96-2ad6-4e2e-d718-acecb1afb7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Conteo de entidades:\n",
            "{'PER': 2, 'ORG': 1, 'LOC': 1}\n"
          ]
        }
      ],
      "source": [
        "# üìù ESPACIO PARA TUS EJERCICIOS\n",
        "# Usa esta celda para experimentar y desarrollar tus soluciones\n",
        "\n",
        "# Ejemplo: Funci√≥n para contar entidades por tipo\n",
        "def contar_entidades_por_tipo(texto):\n",
        "    \"\"\"Cuenta entidades por categor√≠a usando Transformers\"\"\"\n",
        "    if not ner_pipeline:\n",
        "        return {}\n",
        "\n",
        "    entidades = ner_pipeline(texto)\n",
        "    conteo = {}\n",
        "\n",
        "    for ent in entidades:\n",
        "        tipo = ent['entity_group']\n",
        "        if tipo in conteo:\n",
        "            conteo[tipo] += 1\n",
        "        else:\n",
        "            conteo[tipo] = 1\n",
        "\n",
        "    return conteo\n",
        "\n",
        "# Probar la funci√≥n\n",
        "texto_prueba = \"Juan P√©rez trabaja en Google Argentina en Buenos Aires con Mar√≠a L√≥pez.\"\n",
        "print(\"üìä Conteo de entidades:\")\n",
        "print(contar_entidades_por_tipo(texto_prueba))\n",
        "\n",
        "# TODO: Agrega aqu√≠ tus propias funciones y experimentos"
      ],
      "id": "exercise_template"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "---\n",
        "# üéØ Conclusi√≥n\n",
        "\n",
        "¬°Felicitaciones! Completaste el ejercicio de Reconocimiento de Entidades Nombradas.\n",
        "\n",
        "## üìö Lo que aprendiste:\n",
        "- ‚úÖ Implementar NER con modelos pre-entrenados\n",
        "- ‚úÖ Usar APIs de IA generativa para tareas de PLN\n",
        "- ‚úÖ Crear interfaces interactivas con Gradio\n",
        "- ‚úÖ Comparar diferentes enfoques de NER\n",
        "\n",
        "## üîÑ Pr√≥ximos pasos:\n",
        "1. Experiment√° con otros modelos de Hugging Face\n",
        "2. Prob√° con textos de diferentes dominios\n",
        "3. Implementa tu proyecto integrador\n",
        "4. Compart√≠ tus resultados con la clase\n",
        "\n",
        "## üìñ Recursos adicionales:\n",
        "- [Hugging Face Models](https://huggingface.co/models?pipeline_tag=token-classification&language=es)\n",
        "- [Gradio Documentation](https://gradio.app/docs/)\n",
        "- [Google AI Studio](https://ai.google.dev/)\n",
        "\n",
        "---\n",
        "**¬°√âxito en tu trabajo integrador!** üéìüöÄ"
      ],
      "id": "conclusion_section"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  }
}