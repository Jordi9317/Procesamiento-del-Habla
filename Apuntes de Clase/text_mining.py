# -*- coding: utf-8 -*-
"""text_mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13dyHWYPLfYe094gRoaLfB3oDaR865rTg

#Text mining

## Visión General del Notebook

En esta notebook vamos a ver un ejemplo básico de **text minning aplicado a cuentos de Hernán Casciari** que están disponibles en internet. El objetivo de text mining es extraer informacion util de una base de datos de texto (corpus). El corpus que veremos aqui esta compuesto por los cuentos que Casciari fue subiendo a su blog, en forma de posteos, a lo largo de los años.

El ejercicio integra conceptos fundamentales de NLP como:

* Preprocesamiento de texto
* Tokenización
* Eliminación de stop words
* Vectorización de texto
* Análisis de frecuencias
* Visualización de datos textuales

Las librerías fundamentales utilizadas son:

* pandas: Para manipulación y análisis de datos estructurados
* re: Para procesamiento de expresiones regulares
* nltk: Natural Language Toolkit, fundamental en NLP
* sklearn: Para vectorización de texto y otras herramientas de ML
* wordcloud: Para visualización de frecuencias de palabras
* matplotlib: Para visualizaciones generales


Para este ejercicio integrador vamos a basarnos en este [tutorial](https://www.aprendemachinelearning.com/ejercicio-nlp-cuentos-de-hernan-casciari-python-espanol/) donde la primera parte se trataba de **scrapear** el blog para obtener los textos. Sin embargo la url del blog ya no está disponible y los cuentos se pueden adquirir [aquí](https://hernancasciari.com/libros/) junto a otros contenidos super interesantes como sus audiolibros (sí, es un excelente narrador también).

La idea entonces es tratar de ver "**que tenia casciari en su cabeza**" cuando escribio estos cuentos desde 2004 hasta 2015

# Introducción al Text Mining con Cuentos de Hernán Casciari

## Text Mining

El text mining o minería de texto es una técnica que permite extraer información valiosa y patrones significativos de grandes volúmenes de texto no estructurado. En este notebook, realizaremos un análisis de text mining sobre los cuentos del escritor argentino Hernán Casciari, publicados en su blog entre 2004 y 2015.

## Objetivos del Análisis
- Explorar los patrones temáticos en la escritura de Casciari
- Analizar la evolución de su vocabulario a lo largo del tiempo
- Identificar temas recurrentes en sus cuentos
- Visualizar las palabras más significativas por año

## Datos
Los textos analizados provienen de los cuentos publicados en el blog del autor. Aunque originalmente estos textos fueron scrapeados de su blog, actualmente se pueden encontrar en su sitio web oficial (hernancasciari.com/libros/).

## Estructura del Análisis
1. Carga y preparación de datos
2. Limpieza y preprocesamiento de texto
3. Creación de bag of words
4. Análisis de frecuencias
5. Visualizaciones y análisis temporal

### 1 Cargamos los datos

En vez de conseguir los libros scrapeando la página vamos a cargarlos desde la carpeta de la materia. Son 12 archivos pickleados con terminación .txt correspondientes a los cuentos subidos a su blog en los anios de 2004 a 2015.

A pesar de estar en formato .txt los archivos fueron *pickleados* por lo que debemos cargarlos usando la librería pickle.
"""

# Montamos el drive para los archivos
from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/'

import pickle
import pandas as pd
pd.set_option('max_colwidth', 150) # para que las columnas del dataframe muestren hasta 150 caracteres

"""La carpeta "cuentos_casciari" contiene 12 archivos con los cuentos publicados de Casciari de cada año desde 2004 hasta 2015"""

# Años de publicaciones
anios = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']

# Diccionario para almacenar los datos
data = {}

# Cargar cada archivo
for anio in anios:
    nombre_archivo = path + anio + '.txt'
    try:
        with open(nombre_archivo, "rb") as archivo:
            data[anio] = pickle.load(archivo)
        print(f"Archivo {anio}.txt cargado correctamente")
    except:
        print(f"No se pudo cargar el archivo {anio}.txt")

# Revisamos que se haya guardado bien
print(data.keys())

# Veamos algun trozo de texto
print(data['2004'][0:1000])

"""Al intentar crear un DataFrame utilizando un diccionario donde las llaves son valores escalares (es decir, valores únicos y no iterables, como números o cadenas de texto simples), se produce un error porque pandas espera que cada llave tenga un iterable como valor (como listas o arrays).

Por ejemplo, si tenemos el siguiente diccionario:

data = {
    'fruta1': 'manzana',
    'fruta2': 'banana'
}

Aquí, 'fruta1' y 'fruta2' son claves escalares y sus respectivos valores son cadenas simples. Al intentar crear un DataFrame, obtendremos un error porque pandas no puede interpretar varios registros a partir de este formato.

Para solucionarlo, convertimos los valores del diccionario en listas, creando un nuevo diccionario así:

data_combined = {
    'fruta1': ['manzana'],
    'fruta2': ['banana']
}

Ahora, cada valor es una lista (iterable), lo que permite que pandas construya un DataFrame correctamente.
"""

data.items()

# Nuevo diccionario pasando los valores de las llaves a listas
data_combined = {key: [value] for (key, value) in data.items()}

# Creamos el dataframe
data_df = pd.DataFrame.from_dict(data_combined).transpose() # transponemos para que tenga las dimensiones correctas
data_df.columns = ['transcript']                            # renombramos las columnas (en este caso 1)
data_df = data_df.sort_index()                              # ordenamos los indices de forma creciente
print(data_df)

"""### 2 Limpieza de datos

Ahora aplicaremos algunos de los filtros de limpieza que se suelen usar para poder tratar el texto:

* Pasar texto a minúsculas
* Quitar signos de puntuación (interrogación, y otros símbolos)
* Quitar espacios extra, cambio de carro, tabulaciones

Para eso vamos a ultilizar las librerías "re" y "string"
"""

import re       # libreria de expresiones regulares
import string   # libreria de cadena de caracteres

"""RECORDAMOS: Expresiones regulares usan '\\' para formas especiales, ej: '\n' es salto de linea. si queremos imprimir una cadena de caracteres que sea '\\' seguido de 'n' se usa la notacion de cadena *raw* en Python que es poner una 'r' antes de la cadena de caracteres.

Dicho esto, para encontrar coincidencias en strings en python se utiliza la libreria [re](https://docs.python.org/es/3/library/re.html) de expresiones regulares.

Hay muchisimas expresiones y caracteres especiales que sirven para trabajar con texto. Les invito a buscar en la documentacion mas informacion. Vamos a mostrar aca las mas usuales:

  " . ": coincide con cualquier caracter excepto con una nueva línea

  "\*": Hace que el RE resultante coincida con 0 o más repeticiones del RE precedente, tantas repeticiones como sean posibles. ab* coincidirá con “a”, “ab” o “a” seguido de cualquier número de “b”.

  ?: Hace que la RE resultante coincida con 0 o 1 repeticiones de la RE precedente. ab? coincidirá con “a” o “ab”.

  +: Hace que la RE resultante coincida con 1 o más repeticiones de la RE precedente. ab+ coincidirá con “a” seguido de cualquier número distinto de cero de “b”; no coincidirá solo con “a”.

  [ ]: Se utiliza para indicar un conjunto de caracteres

También se utiliza [string](https://docs.python.org/es/3/library/string.html) que tiene metodos que permiten acceder a distintos tipos de caracteres. Un ejemplo es acceder a los signos de puntuacion con `string.punctuation`
"""

print(string.punctuation)
print(re.escape(string.punctuation))

# Defino una funcion que recibe un texto y devuelve el mismo texto sin singnos,
def clean_text_round1(text):
    # pasa las mayusculas del texto a minusculas
    text = text.lower()
    # reemplaza texto entre corchetes por espacio en blanco.. ¿ y \% no se..
    text = re.sub('\[.*?¿\]\%', ' ', text)
    # reemplaza singnos de puntuacion por espacio en blanco.. %s -> \S+ es cualquier caracter que no sea un espacio en blanco
    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)
    # remueve palabras que contienen numeros.
    text = re.sub('\w*\d\w*', '', text)
    return text

# Defino una funcion anonima que al pasarle un argumento devuelve el resultado de aplicarle la funcion anterior a este mismo argumento
round1 = lambda x: clean_text_round1(x)

# Dataframe que resulta de aplicarle a las columnas la funcion de limpieza
data_clean = pd.DataFrame(data_df.transcript.apply(round1))

print(data_clean)

data_clean.transcript[0]

# Hacemos una segunda vuelta de limpieza
def clean_text_round2(text):
    # Sacamos comillas, los puntos suspensivos, <<, >>
    text = re.sub('[‘’“”…«»]', '', text)
    text = re.sub('\n', ' ', text)
    return text

round2 = lambda x: clean_text_round2(x)

data_clean = pd.DataFrame(data_clean.transcript.apply(round2))

# Lo pickleamos y guardamos
data_df.to_pickle("/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/corpus.pkl")

print(data_clean)

data_clean.transcript[0]

"""### Creamos nuestro “Bag of Words”

A partir del dataset que limpiamos, creamos y contamos las palabras:
"""

# Vamos a crear la matriz de documentos-terminos usando usando CountVectorizer, y excluiremos las stop words del espaniol
from sklearn.feature_extraction.text import CountVectorizer

# Cargamos del paquete nltk las stopwords del espaniol a la lista "lines"

import nltk
nltk.download('stopwords') # hay que descargar este modulo en particular

lines = nltk.corpus.stopwords.words('spanish')

print(lines)

"""Ahora vamos a crear la matriz de terminos-documentos. Para ello hay una funcion de scikit-learn que si le das una secuencia de items de tipo string o byte hace exactamente eso."""

# Inicializo el modelo excluyendo stopwords
cv = CountVectorizer(stop_words=lines)
# Ajustamos el modelo y lo aplicamos al texto de nuestro dataframe generando una matriz esparsa o matriz dispersa
data_cv = cv.fit_transform(data_clean.transcript)
# Nos creamos un dataframe transformando a densa la matriz generada recien que tiene como columnas las palabras (terminos) y como filas los documentos
data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())
# Le asignamos los indices del dataframe anterior
data_dtm.index = data_clean.index

"""En el contexto del procesamiento de texto y el análisis de datos, la matriz esparsa es una herramienta fundamental. Esta matriz surge del deseo de representar documentos como vectores, donde cada dimensión (o columna) se corresponde con una palabra del vocabulario.

La razón por la que utilizamos matrices esparsas es que en la mayoría de los casos, un documento contiene solo un pequeño subconjunto del vocabulario total. Por ejemplo, si tenemos un corpus con miles de palabras pero un documento particular solo utiliza unas pocas, la matriz resultante de la representación será mayormente llena de ceros.

Ejemplo:

Supongamos que tenemos 3 documentos y un vocabulario de 4 palabras:

Documentos:

* 1: "manzana"
* 2: "banana"
* 3: "manzana banana"

Vocabulario: ["manzana", "banana", "uva", "pera"]

La matriz resultante sería:

        manzana  banana  uva  pera
     
     1       1       0    0     0   # Documento 1
     
     2       0       1    0     0   # Documento 2
     
     3       1       1    0     0   # Documento 3


Esta representación eficiente tiene varias ventajas:

1. **Ahorro de espacio**: Almacenar solo los valores no cero y sus posiciones ahorra considerablemente espacio en disco y memoria.

2. **Rendimiento**: Muchas operaciones matemáticas en matrices esparsas se pueden hacer de manera más rápida y eficiente, lo que es crucial para manejar grandes volúmenes de texto.

Como resultado, al ajustar el modelo (como `CountVectorizer` o `TF-IDF`), generamos una matriz esparsa que nos permite realizar análisis más complejos sin perder rendimiento ni eficiencia.
"""

data_cv

data_dtm

"""----
Vemos que hay palabras que no estan en el idioma espaniol como **českomoravský**. Dependiendo la informacion que queramos obtener con estos datos entonces tal vez es conveniente filtrar este tipo de palabras. Como lo que vamos a analizar en esta notebook es el vocabulario del escritor, en particular el numero de palabras unicas, entonces las vamos a dejar.

**nltk** tiene [corpus](https://www.nltk.org/book/ch02.html) que se pueden cargar y utilizar para filtrar palabras de distintos idiomas o nombres propios.

```
# Ejemplo de palabras en ingles
english_vocab = set(w.lower() for w in nltk.corpus.words.words())
```

Ojo! que hay palabras que estan en ingles pero no estan en un *diccionario*, como expresiones tipicas de chats, algunos nombres propios o abreviaturas. Entonces hay que tener cuidado cuando pasamos de mayusculas a minusculas. Si las queremos filtrar entonces tenemos que ser cuidadosxs con el corpus que usamos.

---

Guardo los datos generados
"""

# Guardamos en formato pickle los dataframes
data_dtm.to_pickle("/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/dtm.pkl")
data_clean.to_pickle('/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/data_clean.pkl')
# Guardamos tambien el objeto CountVectorize
pickle.dump(cv, open("/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/cv.pkl", "wb"))

"""### 3 Análisis exploratorio

Vamos a explorar el dataset
"""

# Cargamos el dataframe que generamos previamente
data = pd.read_pickle('/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/dtm.pkl')

# Transponemos para tener una matriz de terminosXdocumentos
data = data.transpose()

data.head()

"""###  Palabras más usadas por año

veamos las palabras más usadas cada año:
"""

# Creo un diccionario
top_dict = {}

# Por cada año
for c in data.columns:
    top = data[c].sort_values(ascending=False).head(30) # Ordeno las filas en forma decreciente y me quedo con las 30 palabras mas usadas
    top_dict[c]= list(zip(top.index, top.values))       # le asigno el año a la key del diccionario y como valor una tupla con la palabra y su frecuencia

print(top_dict)
print("\n")

# Imprimo las 15 palabras mas frecuentes por año
for anio, top_words in top_dict.items():
    print(anio) # imprimo la key
    print(', '.join([word for word, count in top_words[0:14]])) # imprimo las palabras en orden decreciente segun frecuencia y separadas con espacio y coma

"""Vemos que hay nombres propios, numeros escrito con letras y algunas palabras como el adverbio "tan" que no nos da informacion por si solo. Filtramos estas palabras.

Tambien vemos que hay palabras que se repiten dentro de las mas frecuentes de cada anio en todos los *anios* que tambien queremos filtrar

### Agregamos Stop Words

Vamos a unir las 12 listas de palabras mas usadas en un nuevo ranking y de esas, tomaremos las "mas usadas" para agregar al listado de Stop Words. Es decir, si una palabra aparecio en el top 30 mas de la mitad de las veces (en 7 anios o mas) entonces la considero una stop word.

Vamos a usar la funcion Counter de la libreria collections que dada una lista de strings nos devuelve la cantidad de repeticiones de cada elemento
"""

from collections import Counter

# Generamos una lista con las 30 palabras mas usadas de cada anio
words = []
for anio in data.columns:
    top = [word for (word, count) in top_dict[anio]] # armo una lista quedandome solo con las palabras del diccionario que creamos antes
    for t in top:
        words.append(t)                              # agrego cada palabra a la lista vacia, por separado

# usando el metodo .most_common() obtengo las palabras mas usadas, y la cantidad de veces que fue agregada a la lista words
print(Counter(words).most_common())

# Creo una lista de nuevas stop words considerando como stop words a palabras que recibieron mas de la mitad de las cuentas posibles (12 anios)
add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]
print(add_stop_words)

"""### Actualizamos nuestra Bag of Words

Ahora quitaremos las Stop words de nuestro dataset. Usaremos el listado de stop words en espaniol de nltk, el que generamos recién y uno adicional que tiene en cuenta las observaciones que palabras fueron las mas frecuentes
Ojo! esto puede parecer **arbitrario** y en parte lo es!

Entonces volveremos a cargar los datos limpios y cuando filtramos las stop words agregamos estas dos listas nuevas
"""

from sklearn.feature_extraction import text

# Leemos los datos limpios
data_clean = pd.read_pickle('/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/data_clean.pkl')

# Agregamos la lista de stop words "lines" las nuevas stop words
for pal in add_stop_words:
    lines.append(pal)

# Creamos la lista que tiene algunos nombres propios que aparecieron entre las palabras mas frecuentes y otro tipo de palabras
more_stop_words=['alex','lucas','andrés','mirta','tres','primer','primera','dos','uno','veces', 'así', 'luego', 'quizá','cosa','cosas','tan','asi','andres','todas','sólo','jesús','pablo','pepe']
for pal in more_stop_words:
    lines.append(pal)

# Recreamos la matriz de documentos y terminos pero ustando la nueva lista "mejorada" de stopwords
cv = CountVectorizer(stop_words=lines)
data_cv = cv.fit_transform(data_clean.transcript)
# Use get_feature_names_out() instead of get_feature_names()
data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())
data_stop.index = data_clean.index

# Lo guardamos en formato pickle
pickle.dump(cv, open("/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/cv_stop.pkl", "wb"))
data_stop.to_pickle("/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/dtm_stop.pkl")

"""### Nube de Palabras

Queremos ver “que tenia Hernan Casciari en su cabeza” entre 2004 y 2015 en sus cuentos usando WordClouds.
"""

from wordcloud import WordCloud # importo la funcion WordCloud
import matplotlib.pyplot as plt

# Creo el objeto WordCloud con determinados parametros y utilizando nuestra lista de stopwords
wc = WordCloud(stopwords=lines, background_color="white", colormap="Dark2",
               max_font_size=150, random_state=42)

plt.rcParams['figure.figsize'] = [16,12] # tamanio de los plots

# Para cada anio creo un WC
for index, anio in enumerate(data.columns):
    wc.generate(data_clean.transcript[anio])  # aca le pido que genere los WC a partir del texto de cada anio
    plt.subplot(4, 3, index+1)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(anios[index])
plt.show()

"""La hija de Hernan Casciari nacio en el 2004. Se puede tratar de analizar como el nacimiento de su hija influencio su escritura. En el 2007, cuando ella tenia tres anios, vemos que empezo a tener mas importancia la palabra "padre" en sus cuentos. Y cuando ella tenia once anios, en el 2015, es la palabra mas frecuente. Siguiendo este analisis se puede pensar que verla crecer afecto enormemente el contenido de sus cuentos.

Siguiendo este analisis interpretativo se nota en estas wordclouds algunos topicos importantes que definen su personalidad. Es un escritor que le gusta mucho escribir sobre su vida, por lo que no es de extraniarse que "libro", "revista" u "orsai", su editorial, aparezcan en muchas WCs. Tambien sabemos que es un fanatico del "futbol", y que es muy familiero ("padre", "madre", "hija")

### Estadísticas de Palabras por año

Ahora sacaremos algunas estadísticas de palabras únicas por año (el tamaño del vocabulario empleado) y el promedio de palabras por artículo
"""

data[data.columns[1]].to_numpy

# Encontraremos las palabras unicas por anio identificando los elementos non-zero en la matriz de documentos y terminos
unique_list = []
for anio in data.columns:
    uniques = data[anio].to_numpy().nonzero()[0].size # tengo que transformar a un array para aplicar la funcion nonzero
    unique_list.append(uniques)

# Creo un nuevo dataframe con el numero de palabras unicas por anio
data_words = pd.DataFrame(list(zip(anios, unique_list)), columns=['Anio', 'unique_words'])
#data_unique_sort = data_words.sort_values(by='unique_words')
data_unique_sort = data_words # sin ordenar
print(data_unique_sort)

# El numero de post por anio, esto es dato
posts_per_year = [50, 27, 18, 50, 42, 22, 50, 33, 31, 17, 33, 13]

# Encuentro el numero total de palabras por anio
total_list = []
for anio in data.columns:
    totals = sum(data[anio])
    total_list.append(totals)

# Agrego las columnas al dataframe
data_words['total_words'] = total_list
data_words['posts_per_year'] = posts_per_year
data_words['words_per_posts'] = data_words['total_words'] / data_words['posts_per_year']

data_wpm_sort = data_words #sin ordenar
print(data_wpm_sort)

"""##### Visualización de la tabla

Veamos los datos en gráfico de barras horizontales:
"""

import numpy as np
plt.rcParams['figure.figsize'] = [16, 6]

y_pos = np.arange(len(data_words))

plt.subplot(1, 3, 1)
plt.barh(y_pos,posts_per_year, align='center')
plt.yticks(y_pos, anios)
plt.title('# de Posts', fontsize=20)


plt.subplot(1, 3, 2)
plt.barh(y_pos, data_unique_sort.unique_words, align='center')
plt.yticks(y_pos, data_unique_sort.Anio)
plt.title('# palabras unicas', fontsize=20)

plt.subplot(1, 3, 3)
plt.barh(y_pos, data_wpm_sort.words_per_posts, align='center')
plt.yticks(y_pos, data_wpm_sort.Anio)
plt.title('# palabras por post', fontsize=20)

plt.tight_layout()
plt.show()

"""Y hagamos una comparativa de frecuencia de uso de algunas palabras (se pueden elegir cualquiera)"""

import nltk
from nltk.corpus import PlaintextCorpusReader

corpus_root = "/content/drive/MyDrive/Clases/HABLA/003/PRA/cuentos_casciari/"
wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')
#wordlists.fileids() # con esto listamos los archivos del directorio

cfd = nltk.ConditionalFreqDist(
        (word,genre)
        for genre in anios
        for w in wordlists.words(genre + '.txt')
        for word in ['casa','futbol','tiempo','vida']
        if w.lower().startswith(word) )
cfd.plot()

"""## Conclusiones

Repasemos lo que hicimos y que resultados sacamos:

* Limpiamos los textos, quitamos caracteres que no utilizamos y creamos un listado de stop_words (palabras para omitir)

* Exploración de datos:
Realizamos estadísticas básicas, como el vocabulario usado, cantidad de palabras por año y promedio por posts.

* Creamos Nubes de Palabras por año ya que es una manera de visualizar textos y frecuencia de termino a lo largo de los anios (similar a [Google Ngrams Viewer](https://books.google.com/ngrams)) falta dividir por la cantidad total de palabras de cada anio
"""