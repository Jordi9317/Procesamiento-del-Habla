# -*- coding: utf-8 -*-
"""web_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uoRwkka1togkfBZGXZY_F_QiMpb4O51s
"""

import requests
from bs4 import BeautifulSoup

"""**requests:** Esta biblioteca se utiliza para hacer solicitudes HTTP a páginas web.

**BeautifulSoup:** Esta biblioteca permite analizar (parsear) documentos HTML y XML, facilitando la extracción de información de ellos.

##Solicitud y análisis de una página web
"""

contenido = requests.get("https://es.wikipedia.org/wiki/Categor%C3%ADa:Pel%C3%ADculas_ganadoras_del_premio_%C3%93scar_a_la_mejor_pel%C3%ADcula").text
soup = BeautifulSoup(contenido, "html.parser")

"""**requests.get(...):** Realiza una solicitud GET a la URL proporcionada, que en este caso es la categoría de películas ganadoras del Óscar.  
**.text:** Obtiene el contenido de la respuesta en texto plano.  
**BeautifulSoup(contenido, "html.parser"):** Crea un objeto BeautifulSoup que permite manipular y extraer datos del HTML de la página.
"""

#print(contenido)

#soup.find_all("a")

"""##Extracción de enlaces"""

for a in soup.find_all("a", href=True):
  print("'https://en.wikipedia.org/" + a["href"] + "' ,")
  #print(a["href"])

"""**soup.find_all("a", href=True):** Busca todos los elementos `<a>` (enlaces) dentro del HTML que tengan un atributo `href`.
**print(...):** Imprime cada enlace encontrado. Los enlaces serán relativos, por lo que se concatenan con `https://en.wikipedia.org/` para formar una URL completa.

##Definición de una lista de URLs
"""

lista = []

"""La lista contiene múltiples URLs, algunas de las cuales parecen repetidas o erróneas (por ejemplo, incluyen múltiples esquemas `https://` o enlaces no válidos).
Es importante limpiar y depurar esta lista para asegurar que se estén utilizando URLs válidas en el siguiente análisis.

##Extracción de textos de varios enlaces
"""

text = " "

for item in lista:
  texto_html = requests.get(item).text
  soup = BeautifulSoup(texto_html, "html.parser")
  for paragraphe in soup.find_all("p"):
    text += str(paragraphe.getText())

print(text)

"""**Extracción de textos de varios enlaces**

Se inicializa una variable `text` como cadena vacía.
Se itera sobre cada elemento de la lista para realizar solicitudes HTTP.
Se utiliza BeautifulSoup para parsear cada página y extraer texto de los párrafos (`<p>`).
Se concatenan todos los textos extraídos en la variable `text`.

##Conteo de palabras
"""

from collections import Counter
import re

lista_de_palabras = re.sub("[^\w]", " ", text).split()
contador = Counter(lista_de_palabras)

print(contador)

"""**re.sub("[^\w]", " ", text):** Reemplaza todos los caracteres que no son letras o números en `text` con espacios.
**.split():** Divide el texto en palabras, creando una lista de palabras.
**Counter(lista_de_palabras):** Crea un contador que calcula la frecuencia de cada palabra en la lista.
**print(contador):** Muestra la cantidad de veces que aparece cada palabra.

## Conclusiones y Recomendaciones

Este cuaderno realiza un scraping básico de una categoría de Wikipedia, ideal para aprender conceptos de extracción de datos y manipulación de texto.

### Recomendaciones para mejorar el código:

* **Profundizar en técnicas de limpieza de datos para mejorar la calidad del análisis final:** Explorar bibliotecas como `NLTK` o `spaCy` para realizar un preprocesamiento más avanzado del texto, incluyendo la eliminación de stop words, stemming o lematización.
"""